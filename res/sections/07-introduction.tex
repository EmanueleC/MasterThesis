\newpage
\begin{center}
\section*{Acronyms}
\begin{acronym}
\acro{IR}{Information Retrieval}
\acro{DRMM}{Deep Relevance Matching Model}
\acro{BIM}{Binary Independence Model}
\acro{QL}{Query Likelihood}
\acro{MAP}{Mean Average Precision}
\acro{nDCG}{Normalized Discounted Cumulative Gain}
\acro{IDF}{Inverse Document Frequency}
\acro{TREC}{Text REtrieval Conference}
\acro{LTR}{Learning To Rank}
\acro{MLP}{Multi Layer Perceptron}
\acro{W2V}{Word2Vec}
\acro{CBOW}{Continuous Bag Of Words}
\acro{GloVe}{Global Vectors}
\acro{PRF}{Pseudo Relevance Feedback}
\acro{PRIMAD}{Platform, Research goal, Implementation, Method, Actor, Data}
\acro{OOV}{Out Of Vocabulary}
\end{acronym}
\end{center}

\newpage
\chapter{Introduction}

In recent years, there have been dramatic improvements in performance in computer vision, speech recognition, and machine translation tasks.

These improvements were possible thanks to the combination of three factors: (i) advancement in the study of neural network models; (ii) the availability of large datasets and (iii) increased computing power.

In this context, the Information Retrieval (IR) community has just begun to explore neural networks models with the purpose of verifying whether they can be beneficial also in some popular IR tasks (e.g. document ad-hoc retrieval).

Starting from 2013, a new field was born from the intersection of IR and Neural networks: Neural IR, appealing researchers and students.

Although some Neural IR models have indeed produced some improvements over the baselines (w.r.t. document ad-hoc retrieval), the consequence of their application to IR have not yet been understood.

There is a strong discussion going on whether IR needs ``Neural IR'' or not. In fact, there are two main problems with these models: one regards efficiency (especially when a large collection of documents is involved), the other regards their ability to learn in a way that can address the complexity of IR tasks (where the concept of \textit{relevance} plays an important role).

The first problem arises from the long time required by a Neural IR model to compute a similarity score between an (appropriate) learnt representation of a document and a query. In case of a large corpus, this time becomes prohibitive.

The second problem is especially linked to the difficulty to learn from queries and documents when no large-scale supervised data is available.

Unfortunately, this is often the case, in fact it is very expensive for a human to label a document ``relevant'' or ``not relevant'' with respect to a certain topic (most of it because relevance is a multifaceted concept).

A couple of strategies have been applied to cope with these problem: pseudo relevance feedback and a re-ranking approach.

Both of them involve the use of a traditional retrieval model in order to obtain supervised data, contributing to the debate on the need of Neural IR.

This work consists of two parts: a bibliographic section and an experimental section.

In the first few chapters I conducted an analysis of the field(s) of study: IR, artificial neural networks and Neural IR.

Then I reviewed several state-of-the-art works in Neural IR and compared them, pointing out their common characteristics and differences.

In the final chapters I replicated the experiment of Guo et al. in \cite{drmm} and critically evaluated it, with a discussion of the results.

Thus, my contributions were: a critical analysis of an emerging (hybrid) field of study (Neural IR), the replication (redoing from scratch) and reproduction of DRMM, a deep relevance matching model, contributing to the IR community's growing interests in reproducibility and related issues; and making the source code publicly available so that others can inspect it.
